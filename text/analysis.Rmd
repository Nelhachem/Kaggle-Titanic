Titanic Kaggle
========================================================

Load both the train and test data
--------------------------------------

```{r, cache=TRUE}
rm(list=ls())
train <- read.csv(file="../data/raw//train.csv", header=T, sep=",")
test <- read.csv(file="../data/raw//test.csv", header=T, sep=",")
```

Initial Analysis
---------------------------
```{r}
summary(train)
summary(test)
```
A few things worth noting based on the summary:

* The Survived variable should be a factor variable with two levels, but we can always use it as a factor using the "as.factor()" method.

* There are a few missing values in Age in both sets. We can fill them with different methods but whatever methods we use must only be based on the train data. The same works for the Fare variable.

* Embarked variable has two missing values in the train data. Since this is a very small portion, we can first fill them with the value "S" (the largest level).

* PClass should probably be a factor variable but there is possibly a ranking in the variable (first class, middle class, and third class). So in order to keep this info, we leave it as is (although this may not precisely represent the difference between two classes). We can think about ordinal encoding but the algorithms we are using in R may not accept ordinal variables.

* We should probably not include PassengerId, Ticket and Cabin in the modeling. But we can always check them later.

* Usually I would treat the Name variable the same as the three variables above. But according to the tutorial on Kaggle, the text in the Name may actually provide something useful. So We can keep it now.

Preprocessing
--------------------------
### Filling the missing values
There are a few different ways for filling missing values. We will start with the simplest one -- fill all missing value with the median value in the train data.

```{r, cache=TRUE}
train.processed <- train
test.processed <- test

# filling the missing value in the Age variable
train.processed$Age[is.na(train.processed$Age)] <- median(train.processed$Age, na.rm=T)
test.processed$Age[is.na(test.processed$Age)] <- median(train.processed$Age, na.rm=T)

# filling the missing value in the Fare variable in test data
test.processed$Fare[is.na(test.processed$Fare)] <- median(train.processed$Fare, 
                                                        na.rm=T)

# fix the missing values in the Embarked variable
# this may be a bit tricky. In order to keep the same level order in the variable
# we have to merge the train and test data together first
test.processed$Survived <- NA
combined <- rbind(train.processed, test.processed)
combined$Embarked[combined$Embarked == ""] <- "S"
combined$Embarked <- factor(combined$Embarked)

# split back to train and test data
train.processed <- combined[1:891,]
test.processed <- combined[-c(1:891),]

save(train.processed, file="../data/processed/train.processed.v1.RData")
save(test.processed, file="../data/processed/test.processed.v1.RData")
```

After this simple preprocessing, let's take a look the summary.
```{r}
summary(train.processed)
summary(test.processed)
```
There are no more missing values except for the Survived variable in the test, which is the target we are going to predict.

Prediction
-----------------------------
### Random Forest
If you haven't install the randomForest package, install it first.
```{r}
# install.packages("randomForest")
require(randomForest)
set.seed(101)

fit <- randomForest(as.factor(Survived) ~ . - PassengerId - Name - Ticket - Cabin,
                    data = train.processed, ntree=2000, importance=T)
```
Let's take a look at the variable importance.
```{r fig.height=4, fig.width=5}
varImpPlot(fit)
```
It seems that the variable Sex is dominating. This may not be good but let's see the prediction of this model first.
```{r, cache=TRUE}
test.processed$Survived <- NULL
Prediction <- predict(fit, test.processed)
submit <- data.frame(PassengerId = test.processed$PassengerId, Survived = Prediction)
write.csv(submit, file = "../results/res.randomforest.v1.csv", row.names = FALSE)
```
Now this submission on April 12th, 2014 achieved 77.512% accuracy, ranking 803rd/1264. Good job submitting the first result. We have a lot space for improvement. Keep going!

Analysis on April 13, 2014
--------------------------------
Yesterday we used the variable median to fill the missing values in the data. What if we use another way to do that instead?

We use the same method in the tutorial--the rpart with anova method.
```{r, cache=TRUE}
# we set the value of original empty cells in Age back to NA
train.processed$Age[is.na(train$Age)] <- NA
test.processed$Age[is.na(test$Age)] <- NA

# install.package("rpart")
require(rpart)
Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked, data=train.processed[!is.na(train.processed$Age),], method="anova")

train.processed$Age[is.na(train.processed$Age)] <- predict(Agefit, train.processed[is.na(train.processed$Age),])
test.processed$Age[is.na(test.processed$Age)] <- predict(Agefit, test.processed[is.na(test.processed$Age),])

save(train.processed, file="../data/processed/train.processed.v2.RData")
save(test.processed, file="../data/processed/test.processed.v2.RData")
```
Prediction
-----------------------------
### Random Forest
If you haven't install the randomForest package, install it first.
```{r,cache=TRUE}
# install.packages("randomForest")
require(randomForest)
set.seed(101)

fit <- randomForest(as.factor(Survived) ~ . - PassengerId - Name - Ticket - Cabin,
                    data = train.processed, ntree=2000, importance=T)
```
Let's take a look at the variable importance.
```{r fig.height=4, fig.width=5}
varImpPlot(fit)
```
It seems that the variable Sex is dominating. This may not be good but let's see the prediction of this model first.
```{r, cache=TRUE}
test.processed$Survived <- NULL
Prediction <- predict(fit, test.processed)
submit <- data.frame(PassengerId = test.processed$PassengerId, Survived = Prediction)
write.csv(submit, file = "../results/res.randomforest.v2.csv", row.names = FALSE)
```
Now this submission on April 13th, 2014 achieved 77.990% accuracy, ranking 741st/1258. Good job submitting the first result. We have a lot space for improvement. Keep going!

Analysis on April 19, 2014
==============================================
(Mentioning about Learning Curve in Andrew Ng's class here) According to the learning curve analysis on Logistic Regression (this will be inserted here), we are likely to be underfitting the data. Therefore, getting more data is not going to help much. However, we can add additional features or polynomial terms to help. 

So what additional features can we add here? Remember that at the beginning, we removed several features including PassengerId, Cabin, Ticket and Name. As mentioned in the tutorial, the first three features are almost unique to each passenger. But what about the Name? Maybe we can extract something useful from those text strings. The tutorial mentioned the Titles of each passenger, which represents their social class and influences how they behaved. So let's extract the title as the tutorial does.

```{r}
test.processed$Survived <- NA
combi <- rbind(train.processed, test.processed)
combi$Name <- as.character(combi$Name)
combi$Title <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
combi$Title <- sub(' ', '', combi$Title)
```
OK, let's take a look at it.
```{r}
table(combi$Title)
```
It appears that there are some rare titles and we need to merge them together.
```{r}
combi$Title[combi$Title %in% c('Mme', 'Mlle')] <- 'Mlle'
combi$Title[combi$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'
combi$Title[combi$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'
combi$Title <- factor(combi$Title)
```
The tutorial also added other features including FamilySize and FamilyID. We add them here as well.
```{r}
combi$FamilySize <- combi$SibSp + combi$Parch + 1
combi$Surname <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][1]})
combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname, sep="")
famIDs <- data.frame(table(combi$FamilyID))
famIDs <- famIDs[famIDs$Freq <= 2,]
combi$FamilyID[combi$FamilyID %in% famIDs$Var1] <- 'Small'
combi$FamilyID <- factor(combi$FamilyID)

combi$Survived <- as.factor(combi$Survived)
train.processed <- combi[1:891,]
test.processed <- combi[892:1309,]
save(train.processed, file="../data/processed/train.processed.v3.RData")
save(test.processed, file="../data/processed/test.processed.v3.RData")
```
Prediction
------------------------
```{r}
fit <- glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize, family = binomial, data = train.processed)

test.processed$Survived <- NULL
Prediction <-ifelse(predict(fit, newdata=test.processed, type="response")>0.5, "1", "0")  
submit <- data.frame(PassengerId = test.processed$PassengerId, Survived = Prediction)
write.csv(submit, file = "../results/res.randomforest.v3.csv", row.names = FALSE)
```
And this achieves the same score 0.77990 as the randomForest we tried before. And it is better than last time when we tried Logistic Regression (around 0.75). This means adding the new features does help!

Analysis on April 21, 2014
======================================
Adding additional features
--------------------------------------
We can consider adding polynomial terms in the model. But it only works with numerical variables. Start with degree 2 polynomials and move on to higher degree later. We can also consider the interaction between variables, for example, the one between Age and Sex (using Age*Sex in the model formula).

```{r}
fit <- glm(Survived ~ Pclass + Sex+ Sex:Age + poly(Age,2) + poly(SibSp,2) + poly(Parch,2) + poly(Fare,2) + Embarked + Title*Sex + I(FamilySize^2), family = binomial, data = train.processed)
summary(fit)
test.processed$Survived <- NULL
Prediction <-ifelse(predict(fit, newdata=test.processed, type="response")>0.5, "1", "0")  
submit <- data.frame(PassengerId = test.processed$PassengerId, Survived = Prediction)
write.csv(submit, file = "../results/res.randomforest.v4.csv", row.names = FALSE)
# require(bestglm)
```
This improved the result with 0.00478 and now we rank at 516/1227. Next step, check if we are overfitting or still underfitting. Either way, consider feature selection to filter out irrelevant or redundant features. 

Well, my wife says dear just try SVM. So here we go, simple SVM without tuning.
```{r}
require(e1071)
svmfit = svm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize, data = train.processed, kernel = "linear", cost = 10, scale = FALSE)
test.processed$Survived <- NULL
Prediction <-predict(svmfit, newdata=test.processed)
submit <- data.frame(PassengerId = test.processed$PassengerId, Survived = Prediction)
write.csv(submit, file = "../results/res.randomforest.v5.csv", row.names = FALSE)
```